{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164f81cc",
   "metadata": {},
   "source": [
    "# Task2: 基于PyTorch框架的手写数字识别\n",
    "## 引入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004bf64b-0af9-4c00-99b2-1b4405e8f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d527ca",
   "metadata": {},
   "source": [
    "## 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc22903",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "epoches = 100\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb7104",
   "metadata": {},
   "source": [
    "## Re-implement MLP\n",
    "利用PyTorch的内置神经网络模块（torch.nn.Module的子类），在MLP类中实现两个函数：\n",
    "+ 在__init__函数中，定义一个网络结构为[784-245-128-10]的MLP模型结构\n",
    "+ 在forward函数中，实现该MLP模型的前向传播过程\n",
    "\n",
    "下面是一些供你参考/可能用到的API函数：\n",
    "\n",
    "- torch.nn.Linear(*in_features*, *out_features*, *bias=True*, *device=None*, *dtype=None*) [\n",
    "  Link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "  - in_features: 输入网络层的特征维度\n",
    "  - out_features: 输出网络层的特征维度\n",
    "- torch.nn.Module.forward(**input*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward)\n",
    "  - 执行模型的前向过程，继承nn.Module类的类实例可以直接通过变量名加括号实现forward函数的调用，不需要写明调用forward函数\n",
    "  - 如定义了MLP(nn.Module)，则对于mlp = MLP()，可以通过mlp(**input*)调用\n",
    "- torch.Tensor.reshape(*shape*) [Link](https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html)\n",
    "  - shape: 当前tensor希望修改为的形状，如(2, 2)或(-1, 3)\n",
    "    - -1指该维度大小根据原数据维度大小和其它给定维度大小计算得到，至多可以给一个-1\n",
    "- torch.nn.Sigmoid() [Link](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1193adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: 定义上文要求的MLP模型结构\n",
    "        self.fc1 = nn.Linear(784, 512) # 输入层→隐藏层1（784→512维）\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # 对隐藏层1输出做归一化处理\n",
    "        self.dropout1 = nn.Dropout(0.3) # 随机丢弃30%的神经元\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256) # 隐藏层1→隐藏层2（512→256维）\n",
    "        self.bn2 = nn.BatchNorm1d(256) # 对隐藏层2输出做归一化处理\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 10) # 隐藏层2→输出层（256→10维）\n",
    "        # 带负斜率的ReLU变体\n",
    "        # 允许负值输入产生微小梯度，避免ReLU在负区梯度为0的问题\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 隐藏层使用 He(Kaiming) 初始化，适配 LeakyReLU 的负斜率(非线性)参数，确保前向传播时激活值的方差稳定\n",
    "        # 模式选择：'fan_in'表示权重方差的计算方式为输入节点个数，保持各层输入方差稳定\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        # 输出层使用 Xavier 初始化，平衡梯度传播，适合线性输出层\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: 定义MLP模型的前向过程\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.relu(self.bn1(self.fc1(x))) # 全连接→BN→激活\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        o = self.dropout2(x) # 输出层不激活\n",
    "\n",
    "        return self.fc3(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0bf5a",
   "metadata": {},
   "source": [
    "## 示例化MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d07082",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ebfa8",
   "metadata": {},
   "source": [
    "## 定义损失函数、优化算法\n",
    "\n",
    "- torch.nn.CrossEntropyLoss(*weight=None*, *size_average=None*, *ignore_index=- 100*, *reduce=None*, *reduction='mean'*, *label_smoothing=0.0*) [Link](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "  - loss.backward(): loss通过特定的计算方式获得，如调用CrossEntropyLoss；对loss执行backward()会为计算图中涉及的tensor反向计算梯度，累积到tensor.grad上\n",
    "- torch.optim.SGD(*params*, *lr=<required parameter>*, *momentum=0*, *dampening=0*, *weight_decay=0*, *nesterov=False*, ***, *maximize=False*, *foreach=None*, *differentiable=False*)  [Link](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "  - params: 需优化的参数Tensor\n",
    "  - lr: 参数优化的学习率\n",
    "  - zero_grad(): 清空相关参数上累积的梯度\n",
    "  - step(): 根据tensor上累积的梯度，进行一次参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae0d1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f3d31",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "- 自动下载MNIST数据集到./MNIST路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17757476-51c0-4d83-bd6b-a052e6c4d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./MNIST\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root=\"./MNIST\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae6c5",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "此处关于loss和optimizer的用法请参考上一段落的API介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c74b85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:04<00:00, 115.14it/s, epoch=0, train_loss=0.559]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.59it/s, epoch=1, train_loss=0.354]\n",
      "100%|██████████| 468/468 [00:04<00:00, 114.90it/s, epoch=2, train_loss=0.379]\n",
      "100%|██████████| 468/468 [00:04<00:00, 113.41it/s, epoch=3, train_loss=0.226]\n",
      "100%|██████████| 468/468 [00:04<00:00, 111.23it/s, epoch=4, train_loss=0.229]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.39it/s, epoch=5, train_loss=0.405]\n",
      "100%|██████████| 468/468 [00:04<00:00, 107.99it/s, epoch=6, train_loss=0.261]\n",
      "100%|██████████| 468/468 [00:04<00:00, 112.48it/s, epoch=7, train_loss=0.208]\n",
      "100%|██████████| 468/468 [00:04<00:00, 111.28it/s, epoch=8, train_loss=0.196] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.12it/s, epoch=9, train_loss=0.268] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.89it/s, epoch=10, train_loss=0.128] \n",
      "100%|██████████| 468/468 [00:04<00:00, 112.87it/s, epoch=11, train_loss=0.164]\n",
      "100%|██████████| 468/468 [00:04<00:00, 113.56it/s, epoch=12, train_loss=0.229] \n",
      "100%|██████████| 468/468 [00:04<00:00, 112.89it/s, epoch=13, train_loss=0.301] \n",
      "100%|██████████| 468/468 [00:04<00:00, 112.39it/s, epoch=14, train_loss=0.136] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.75it/s, epoch=15, train_loss=0.191] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.60it/s, epoch=16, train_loss=0.209] \n",
      "100%|██████████| 468/468 [00:04<00:00, 112.33it/s, epoch=17, train_loss=0.163] \n",
      "100%|██████████| 468/468 [00:04<00:00, 113.79it/s, epoch=18, train_loss=0.294] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.34it/s, epoch=19, train_loss=0.158] \n",
      "100%|██████████| 468/468 [00:04<00:00, 113.24it/s, epoch=20, train_loss=0.132] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.14it/s, epoch=21, train_loss=0.176] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.12it/s, epoch=22, train_loss=0.0874]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.90it/s, epoch=23, train_loss=0.249] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.84it/s, epoch=24, train_loss=0.0498]\n",
      "100%|██████████| 468/468 [00:04<00:00, 111.24it/s, epoch=25, train_loss=0.185] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.28it/s, epoch=26, train_loss=0.218] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.18it/s, epoch=27, train_loss=0.235] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.54it/s, epoch=28, train_loss=0.115] \n",
      "100%|██████████| 468/468 [00:04<00:00, 112.41it/s, epoch=29, train_loss=0.157] \n",
      "100%|██████████| 468/468 [00:04<00:00, 107.78it/s, epoch=30, train_loss=0.0583]\n",
      "100%|██████████| 468/468 [00:04<00:00, 111.22it/s, epoch=31, train_loss=0.134] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.62it/s, epoch=32, train_loss=0.172] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.86it/s, epoch=33, train_loss=0.317] \n",
      "100%|██████████| 468/468 [00:04<00:00, 111.59it/s, epoch=34, train_loss=0.107] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.27it/s, epoch=35, train_loss=0.0935]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.14it/s, epoch=36, train_loss=0.0964]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.24it/s, epoch=37, train_loss=0.0345]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.98it/s, epoch=38, train_loss=0.0806]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.81it/s, epoch=39, train_loss=0.179] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.75it/s, epoch=40, train_loss=0.154] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.41it/s, epoch=41, train_loss=0.0921]\n",
      "100%|██████████| 468/468 [00:04<00:00, 108.85it/s, epoch=42, train_loss=0.0836]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.10it/s, epoch=43, train_loss=0.0925]\n",
      "100%|██████████| 468/468 [00:04<00:00, 107.60it/s, epoch=44, train_loss=0.0929]\n",
      "100%|██████████| 468/468 [00:04<00:00, 108.81it/s, epoch=45, train_loss=0.0944]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.74it/s, epoch=46, train_loss=0.0599]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.08it/s, epoch=47, train_loss=0.157] \n",
      "100%|██████████| 468/468 [00:04<00:00, 108.62it/s, epoch=48, train_loss=0.109] \n",
      "100%|██████████| 468/468 [00:04<00:00, 108.77it/s, epoch=49, train_loss=0.0769]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.42it/s, epoch=50, train_loss=0.0634]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.42it/s, epoch=51, train_loss=0.0884]\n",
      "100%|██████████| 468/468 [00:04<00:00, 108.41it/s, epoch=52, train_loss=0.0832]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.04it/s, epoch=53, train_loss=0.112] \n",
      "100%|██████████| 468/468 [00:04<00:00, 110.82it/s, epoch=54, train_loss=0.0512]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.24it/s, epoch=55, train_loss=0.0767]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.24it/s, epoch=56, train_loss=0.0742]\n",
      "100%|██████████| 468/468 [00:04<00:00, 107.70it/s, epoch=57, train_loss=0.0499]\n",
      "100%|██████████| 468/468 [00:04<00:00, 108.45it/s, epoch=58, train_loss=0.122] \n",
      "100%|██████████| 468/468 [00:04<00:00, 106.04it/s, epoch=59, train_loss=0.129] \n",
      "100%|██████████| 468/468 [00:04<00:00, 108.88it/s, epoch=60, train_loss=0.0965]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.60it/s, epoch=61, train_loss=0.0492]\n",
      "100%|██████████| 468/468 [00:04<00:00, 110.64it/s, epoch=62, train_loss=0.088] \n",
      "100%|██████████| 468/468 [00:05<00:00, 89.71it/s, epoch=63, train_loss=0.0629] \n",
      "100%|██████████| 468/468 [00:05<00:00, 88.40it/s, epoch=64, train_loss=0.172] \n",
      "100%|██████████| 468/468 [00:05<00:00, 85.60it/s, epoch=65, train_loss=0.1]   \n",
      "100%|██████████| 468/468 [00:05<00:00, 87.24it/s, epoch=66, train_loss=0.131] \n",
      "100%|██████████| 468/468 [00:05<00:00, 93.43it/s, epoch=67, train_loss=0.0957] \n",
      "100%|██████████| 468/468 [00:04<00:00, 101.64it/s, epoch=68, train_loss=0.0972]\n",
      "100%|██████████| 468/468 [00:04<00:00, 99.26it/s, epoch=69, train_loss=0.013]  \n",
      "100%|██████████| 468/468 [00:05<00:00, 93.18it/s, epoch=70, train_loss=0.065]  \n",
      "100%|██████████| 468/468 [00:04<00:00, 98.67it/s, epoch=71, train_loss=0.107]  \n",
      "100%|██████████| 468/468 [00:04<00:00, 99.29it/s, epoch=72, train_loss=0.0621] \n",
      "100%|██████████| 468/468 [00:04<00:00, 101.53it/s, epoch=73, train_loss=0.0509]\n",
      "100%|██████████| 468/468 [00:04<00:00, 101.46it/s, epoch=74, train_loss=0.0793]\n",
      "100%|██████████| 468/468 [00:04<00:00, 103.23it/s, epoch=75, train_loss=0.0826]\n",
      "100%|██████████| 468/468 [00:04<00:00, 101.02it/s, epoch=76, train_loss=0.0398]\n",
      "100%|██████████| 468/468 [00:04<00:00, 99.96it/s, epoch=77, train_loss=0.0531] \n",
      "100%|██████████| 468/468 [00:04<00:00, 100.05it/s, epoch=78, train_loss=0.0978]\n",
      "100%|██████████| 468/468 [00:04<00:00, 100.77it/s, epoch=79, train_loss=0.0856]\n",
      "100%|██████████| 468/468 [00:04<00:00, 95.06it/s, epoch=80, train_loss=0.0993] \n",
      "100%|██████████| 468/468 [00:05<00:00, 88.99it/s, epoch=81, train_loss=0.0917] \n",
      "100%|██████████| 468/468 [00:05<00:00, 86.67it/s, epoch=82, train_loss=0.0968] \n",
      "100%|██████████| 468/468 [00:05<00:00, 92.68it/s, epoch=83, train_loss=0.0758] \n",
      "100%|██████████| 468/468 [00:04<00:00, 104.56it/s, epoch=84, train_loss=0.0718]\n",
      "100%|██████████| 468/468 [00:04<00:00, 109.03it/s, epoch=85, train_loss=0.107] \n",
      "100%|██████████| 468/468 [00:04<00:00, 108.76it/s, epoch=86, train_loss=0.0677] \n",
      "100%|██████████| 468/468 [00:04<00:00, 108.50it/s, epoch=87, train_loss=0.0254]\n",
      "100%|██████████| 468/468 [00:04<00:00, 108.38it/s, epoch=88, train_loss=0.111] \n",
      "100%|██████████| 468/468 [00:04<00:00, 101.08it/s, epoch=89, train_loss=0.0617]\n",
      "100%|██████████| 468/468 [00:04<00:00, 97.30it/s, epoch=90, train_loss=0.0542] \n",
      "100%|██████████| 468/468 [00:04<00:00, 103.62it/s, epoch=91, train_loss=0.0291]\n",
      "100%|██████████| 468/468 [00:04<00:00, 100.34it/s, epoch=92, train_loss=0.0931]\n",
      "100%|██████████| 468/468 [00:04<00:00, 99.97it/s, epoch=93, train_loss=0.0559] \n",
      "100%|██████████| 468/468 [00:04<00:00, 94.05it/s, epoch=94, train_loss=0.127]  \n",
      "100%|██████████| 468/468 [00:04<00:00, 103.81it/s, epoch=95, train_loss=0.0444]\n",
      "100%|██████████| 468/468 [00:04<00:00, 106.06it/s, epoch=96, train_loss=0.0556]\n",
      "100%|██████████| 468/468 [00:04<00:00, 106.44it/s, epoch=97, train_loss=0.112] \n",
      "100%|██████████| 468/468 [00:04<00:00, 107.89it/s, epoch=98, train_loss=0.0383] \n",
      "100%|██████████| 468/468 [00:04<00:00, 107.84it/s, epoch=99, train_loss=0.03]  \n"
     ]
    }
   ],
   "source": [
    "mlp.train()\n",
    "\n",
    "for e in range(epoches):\n",
    "    t = tqdm(train_loader)\n",
    "    for img, label in t:\n",
    "        # Forward img and compute loss\n",
    "        pred = mlp(img)\n",
    "        loss = criterion(pred, label)\n",
    "        \n",
    "        # TODO: 基于优化器的使用方法，完成反向梯度传播、参数更新\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t.set_postfix(epoch=e, train_loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060fa17",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "\n",
    "- torch.argmax(*input*, *dim*, *keepdim=False*) [Link](https://pytorch.org/docs/stable/generated/torch.argmax.html)\n",
    "  - input: 计算基于的tensor\n",
    "  - dim: 希望按哪个维度求max下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bb449e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 164.65it/s, test_acc=0.981]\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "\n",
    "correct_cnt, sample_cnt = 0, 0\n",
    "\n",
    "t = tqdm(test_loader)\n",
    "for img, label in t:\n",
    "    # Predict label for img\n",
    "    img = img.reshape(img.shape[0], -1)\n",
    "    pred = mlp(img)\n",
    "    pred_label = pred.argmax(dim=1)\n",
    "    correct_cnt += (pred_label == label).sum().item()\n",
    "    sample_cnt += pred_label.shape[0]\n",
    "\n",
    "    t.set_postfix(test_acc=correct_cnt/sample_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251bbfb",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "\n",
    "- 将完成训练的模型保存到服务器的model/目录下\n",
    "\n",
    "- ModelScope服务器端无法长久保存文件，因此请及时下载、本地保存你完成的代码，以及模型的参数文件（model/mlp.pt）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113ac32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model/'):\n",
    "    os.mkdir('model/')\n",
    "\n",
    "torch.save(mlp.state_dict(), 'model/mlp.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
